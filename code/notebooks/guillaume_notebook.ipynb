{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "\n",
    "#Training params\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "NUM_CLASSES = 120\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "USE_DROPOUT = True\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/train'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH + 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "mpl.style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "#Models\n",
    "from keras import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.applications import ResNet50, VGG16, InceptionV3\n",
    "\n",
    "#Preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import vgg16\n",
    "from keras.applications import inception_v3\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_datagenerators(\n",
    "    preprocessing_function, validation_split=None, target_size=IMAGE_SIZE, \n",
    "    batch_size=BATCH_SIZE, seed=RANDOM_STATE):\n",
    "    \n",
    "    generators = []\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            preprocessing_function=preprocessing_function,\n",
    "            validation_split=validation_split\n",
    "    )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing_function\n",
    "    )\n",
    "    \n",
    "    if validation_split:\n",
    "        #Generator for train data\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "                DATA_PATH + 'train',  # this is the target directory\n",
    "                target_size=target_size,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                class_mode='categorical',\n",
    "                subset='training',\n",
    "                seed=seed\n",
    "        ) \n",
    "\n",
    "        validation_generator = train_datagen.flow_from_directory(\n",
    "                DATA_PATH + 'train',  # this is the target directory\n",
    "                target_size=target_size,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                class_mode='categorical',\n",
    "                subset='validation',\n",
    "                seed=seed\n",
    "        ) \n",
    "        \n",
    "        generators.append(train_generator)\n",
    "        generators.append(validation_generator)\n",
    "    \n",
    "    else:\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "                DATA_PATH + 'train',  # this is the target directory\n",
    "                target_size=target_size,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                class_mode='categorical'\n",
    "        )\n",
    "        generators.append(train_generator)\n",
    "\n",
    "    #Generator for test data\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "            DATA_PATH + 'test',\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            class_mode='categorical',\n",
    "            seed=seed\n",
    "    )\n",
    "    generators.append(test_generator)\n",
    "    \n",
    "    return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    save_callback = keras.callbacks.ModelCheckpoint(\n",
    "      DATA_PATH + 'models/%s.{epoch:02d}-{val_acc:.2f}.hdf5' %model_name, monitor='val_acc', verbose=0, \n",
    "      save_best_only=True, save_weights_only=False, mode='auto', period=2\n",
    "    )\n",
    "    time_callback = TimeHistory()\n",
    "    ealy_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_acc', patience=2)\n",
    "    \n",
    "    return [save_callback, time_callback, ealy_stopping_callback]\n",
    "\n",
    "\n",
    "def compile_model(base_model, learning_rate=5e-4, use_dropout=USE_DROPOUT):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    #Add our own layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    if use_dropout:\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    #Compile model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate, epsilon=None),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, generator, validation_generator):\n",
    "    history = model.fit_generator(generator=generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=EPOCHS,\n",
    "                    #steps_per_epoch= len(generator.classes) // BATCH_SIZE,\n",
    "                    #validation_steps= len(generator.classes) // BATCH_SIZE,\n",
    "                    #workers=4,\n",
    "                    #callbacks = callbacks\n",
    "                    #use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history_performance(history, title='Accuracy and Validation Accuracy'):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "\n",
    "    t = np.linspace(1, len(acc), len(acc)).flatten()\n",
    "    plt.plot(t, acc, 'navy', label='accuracy')\n",
    "    plt.plot(t, val_acc, 'lightblue', label='validation accuracy')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=4)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_inference_time(model, generator, n_inferences=10):\n",
    "    start = time.time()\n",
    "    \n",
    "    model.predict_generator(generator, n_inferences)\n",
    "    \n",
    "    total_inference_time = time.time() - start\n",
    "    mean_inference_time = total_inference_time / (n_inferences * BATCH_SIZE)\n",
    "    \n",
    "    return mean_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_stats(model_stats, model_name, history, time_callback, test_accuracy, inference_time):\n",
    "    model_stats['models'].append(model_name)\n",
    "    \n",
    "    #Accuracy & validation accuracy\n",
    "    acc = history.history['acc'][-1]\n",
    "    model_stats['accuracy'].append(acc * 100)\n",
    "    model_stats['test_accuracy'].append(test_accuracy * 100)\n",
    "    \n",
    "    #Mean epoch training time\n",
    "    epoch_training_time = np.mean(time_callback.times[1:])\n",
    "    model_stats['epoch_training_time'].append(epoch_training_time)\n",
    "    \n",
    "    #Inference time\n",
    "    model_stats['inference_time_10000pic'].append(inference_time * 10000)\n",
    "    \n",
    "    #Print results\n",
    "    print('Accuracy: %0.1f%% / Test accuracy: %0.1f%%' %(acc*100, test_accuracy*100))\n",
    "    print('Mean epoch training time: %is' %int(epoch_training_time))\n",
    "    print('Mean 10000 inference time: %0.1fs' %(inference_time*10000))\n",
    "    \n",
    "    return model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_stats(stats, figsize=(6, 6)):\n",
    "    blues_cm = cm.Blues\n",
    "    width = 0.2\n",
    "    ind = np.arange(3)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.barh(ind + width, stats['accuracy'], width, color=blues_cm(0.9), label=\"accuracy\", edgecolor='white')\n",
    "    ax.barh(ind + 2 * width, stats['test_accuracy'], width, color=blues_cm(0.7),  label=\"test accuracy\", edgecolor='white')\n",
    "    ax.barh(ind + 3 * width, stats['inference_time_10000pic'], width, color=blues_cm(0.3),  label=\"inference time - 10000 pictures (s)\", edgecolor='white')\n",
    "    ax.barh(ind + 4 * width, stats['epoch_training_time'], width, color=blues_cm(0.1),  label=\"epoch training time (s)\", edgecolor='white')\n",
    "\n",
    "\n",
    "    ax.set_title('Performances for each deep learning model')\n",
    "    ax.set_yticks(ind + width * 3)\n",
    "    ax.set_yticklabels(stats['models'])\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[::-1], labels[::-1], title='Metrics', loc='lower right')\n",
    "    \n",
    "    \n",
    "def plot_models_accs(accs, figsize=(6, 6)):\n",
    "    blues_cm = cm.Blues\n",
    "    width = 0.2\n",
    "    ind = np.arange(3)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.barh(ind + width, accs['acc'], width, color=blues_cm(0.9), label=\"accuracy\", edgecolor='white')\n",
    "    ax.barh(ind + 2 * width, accs['test_accuracy'], width, color=blues_cm(0.7),  label=\"test accuracy\", edgecolor='white')\n",
    "\n",
    "\n",
    "    ax.set_title('Performances for each deep learning model')\n",
    "    ax.set_yticks(ind + width * 3)\n",
    "    ax.set_yticklabels(stats['models'])\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[::-1], labels[::-1], title='Metrics', loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dog_breeds(generator):\n",
    "    dog_breeds = generator.class_indices\n",
    "    dog_breeds = [k for k in dog_breeds.keys()]\n",
    "    dog_breeds.sort()\n",
    "\n",
    "    with open(DATA_PATH + 'breed_list.pickle', 'wb') as fp:\n",
    "        pickle.dump(dog_breeds, fp)\n",
    "\n",
    "    return dog_breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False, plot_values=False, printed=False,\n",
    "                          title='Confusion matrix',\n",
    "                          figsize=(20,20), cmap=plt.cm.Blues, rotation_xticks=90):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    if printed:\n",
    "        print(cm)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=rotation_xticks)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if plot_values:\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            val = format(cm[i, j], fmt)\n",
    "            val = int(float(val)) if (float(val) >= 1.) else val\n",
    "            if float(val):\n",
    "                plt.text(j, i, val,\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = {\n",
    "    'models': [],\n",
    "    'accuracy': [],\n",
    "    'test_accuracy': [],\n",
    "    'epoch_training_time': [],\n",
    "    'inference_time_10000pic': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callbacks\n",
    "vgg16_callbacks = get_callbacks('vgg16')\n",
    "\n",
    "#Compile models without top and with our own layers\n",
    "base_model = VGG16(include_top=False, weights=\"imagenet\", input_shape=INPUT_SHAPE)\n",
    "vgg16_model = compile_model(base_model, learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 120)               123000    \n",
      "=================================================================\n",
      "Total params: 15,363,000\n",
      "Trainable params: 648,312\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7facc0744240>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = validation_generator.next()\n",
    "test = x[63]\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13137 images belonging to 120 classes.\n",
      "Found 1394 images belonging to 120 classes.\n",
      "Found 1817 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "#Get generators\n",
    "train_generator, validation_generator, test_generator = get_test_train_datagenerators(\n",
    "    vgg16.preprocess_input, validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-2606741200f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m vgg16_model, vgg16_history = train_model(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvgg16_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-96-ca2dcb73efcc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, generator, validation_generator)\u001b[0m\n\u001b[1;32m     33\u001b[0m     history = model.fit_generator(generator=generator,\n\u001b[1;32m     34\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0;31m#steps_per_epoch= len(generator.classes) // BATCH_SIZE,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;31m#validation_steps= len(generator.classes) // BATCH_SIZE,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    315\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvhat_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                 \u001b[0mp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm_t\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m           y = ops.convert_to_tensor_v2(\n\u001b[0;32m--> 906\u001b[0;31m               y, dtype_hint=x.dtype.base_dtype, name=\"y\")\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m           \u001b[0;31m# If the RHS is not a tensor, it might be a tensor aware object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1254\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    315\u001b[0m                                          as_ref=False):\n\u001b[1;32m    316\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 258\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    294\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    295\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    297\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m   const_tensor = g._create_op_internal(  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/.virtualenvs/py3cv4/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "vgg16_model, vgg16_history = train_model(\n",
    "    vgg16_model, train_generator, validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
