{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all libraries\n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120 total dog categories.\n",
      "There are 20580 total dog images.\n",
      "\n",
      "There are 14531 training dog images.\n",
      "There are 4232 validation dog images.\n",
      "There are 1817 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 120)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('../data/train')\n",
    "valid_files, valid_targets = load_dataset('../data/validation')\n",
    "test_files, test_targets = load_dataset('../data/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"../data/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bayet/.virtualenvs/py3cv4/lib/python3.5/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "pretrained_model= ResNet50(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)) \n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = pretrained_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "#dropout\n",
    "predictions = Dense(120, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs = pretrained_model.input, outputs=predictions) \n",
    "\n",
    "base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14531 images belonging to 120 classes.\n",
      "Found 4232 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# training image augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import optimizers\n",
    "from keras.callbacks import History \n",
    "from keras.applications import vgg16\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration I will use for training\n",
    "train_datagen = ImageDataGenerator(rotation_range=20,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest',\n",
    "                                   preprocessing_function=vgg16.preprocess_input)\n",
    "\n",
    "# This is the augmentation configuration I will use for testing/validation... just a rescale\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n",
    "\n",
    "# This is the generator which will read pictures found in my training subset\n",
    "train_generator = train_datagen.flow_from_directory('../data/train/',\n",
    "                                                    target_size = (224, 224),\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode = 'categorical',\n",
    "                                                    seed=42)\n",
    "\n",
    "# This is the generator for validation data\n",
    "validation_generator = test_datagen.flow_from_directory('../data/validation/',\n",
    "                                                        target_size = (224, 224),\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, History\n",
    "\n",
    "history = History()\n",
    "checkpoint_augmentation = '../data/checkpoint_augmentation/resnet'\n",
    "model_checkpoint_augmentation = ModelCheckpoint(filepath=checkpoint_augmentation,\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2127s - loss: 1.2266 - accuracy: 0.6378 - val_loss: 0.9512 - val_accuracy: 0.7006\n",
      "Epoch 2/10\n",
      " - 2143s - loss: 1.0626 - accuracy: 0.6788 - val_loss: 0.4939 - val_accuracy: 0.6940\n",
      "Epoch 3/10\n",
      " - 2066s - loss: 1.0005 - accuracy: 0.6966 - val_loss: 1.2360 - val_accuracy: 0.7098\n",
      "Epoch 4/10\n",
      " - 2114s - loss: 0.9050 - accuracy: 0.7245 - val_loss: 1.5541 - val_accuracy: 0.6966\n",
      "Epoch 5/10\n",
      " - 3246s - loss: 0.8651 - accuracy: 0.7379 - val_loss: 1.5739 - val_accuracy: 0.6928\n",
      "Epoch 6/10\n",
      " - 2724s - loss: 0.7973 - accuracy: 0.7551 - val_loss: 1.4875 - val_accuracy: 0.7034\n",
      "Epoch 7/10\n",
      " - 4366s - loss: 0.7530 - accuracy: 0.7638 - val_loss: 0.9672 - val_accuracy: 0.6997\n",
      "Epoch 8/10\n",
      " - 4181s - loss: 0.7141 - accuracy: 0.7759 - val_loss: 0.3811 - val_accuracy: 0.7155\n",
      "Epoch 9/10\n",
      " - 2153s - loss: 0.6715 - accuracy: 0.7921 - val_loss: 3.5684 - val_accuracy: 0.7101\n",
      "Epoch 10/10\n",
      " - 2133s - loss: 0.6676 - accuracy: 0.7903 - val_loss: 3.4774 - val_accuracy: 0.6935\n",
      "CPU times: user 1d 4h 55min 58s, sys: 7h 5min 14s, total: 1d 12h 1min 13s\n",
      "Wall time: 7h 34min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f16ac2aab38>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(train_generator,\n",
    "                    epochs = 10,\n",
    "                    validation_data = validation_generator,\n",
    "                    #callbacks = [model_checkpoint_augmentation, history],\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
